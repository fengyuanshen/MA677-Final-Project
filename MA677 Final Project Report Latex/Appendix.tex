\section{Appendix}

\subsection{R Code for Implementation of the James-Stein Estimator}

\begin{verbatim}
# Function to apply James-Stein Estimator
james_stein_estimator <- function(z, sigma_squared) {
  n <- length(z)
  z_bar <- mean(z)
  s_squared <- sum((z - z_bar)^2) / n
  shrinkage_factor <- 1 - ((n-3) * sigma_squared / s_squared)
  shrinkage_factor <- max(0, shrinkage_factor)  # Ensure non-negative shrinkage
  return(z_bar + shrinkage_factor * (z - z_bar))
}

# Simulate data
set.seed(1)
z <- rnorm(10, mean = 5, sd = 1)
sigma_squared <- 1

# Apply the JSE
jse_results <- james_stein_estimator(z, sigma_squared)
print(jse_results)

data <- data.frame(Index = 1:length(z),
                   Original = z,
                   JamesStein = jse_results)

ggplot(data) +
  geom_point(aes(x = Index, y = Original,
                            color = "Original Data"), pch = 19) +
  geom_point(aes(x = Index, y = JamesStein,
                            color = "James-Stein Estimation"), pch = 8) +
  scale_color_manual(values = c("Original Data" = "#d62828",
                                "James-Stein Estimation" = "#003049")) +
  labs(x = "Index", y = "Estimates", title = "Effect of James-Stein Shrinkage") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.title = element_blank(),
    legend.position = 'bottom'
    ) +
  ylim(2, 8)
\end{verbatim}

\subsection{R Code for Simulation of Estimator Effectiveness}

\begin{verbatim}
# Perform simulations and calculate MLE and JSE
simulate_estimators <- function(n, mu = 5, sigma = 1, num_sim = 1000) {
  mle_errors <- numeric(num_sim)
  jse_errors <- numeric(num_sim)
  
  for (i in 1:num_sim) {
    # Simulate data
    z <- rnorm(n, mean = mu, sd = sigma)
    mle <- mean(z)
    s_squared <- sum((z - mle)^2) / n
    shrinkage_factor <- 1 - ((n-3) * sigma^2 / s_squared)
    shrinkage_factor <- max(0, shrinkage_factor)  # Ensuring non-negative shrinkage
    jse <- mle + shrinkage_factor * (z - mle)
    
    # Calculate errors
    mle_errors[i] <- sum((mle - mu)^2)
    jse_errors[i] <- sum((jse - mu)^2)
  }
  
  data.frame(
    Error = c(jse_errors, mle_errors),
    Estimator = rep(c("err_MLE", "err_JSE"), each = num_sim),
    N = rep(n, 2 * num_sim)
  )
}

# Parameters
sample_sizes <- c(3, 5, 10, 20)
num_simulations <- 1000

# Run simulations for multiple sample sizes
results <- do.call(rbind, lapply(sample_sizes, function(n) {
  simulate_estimators(n = n, num_sim = num_simulations)
}))
results$Estimator <- factor(results$Estimator, levels = c("err_MLE", "err_JSE"))

# Plot
ggplot(results, aes(x = Estimator, y = log(Error + 1), fill = Estimator)) +
  geom_boxplot() +
  facet_wrap(~ N, scales = "free") +
  labs(title = "Comparison of MLE and Jamesâ€“Stein Estimator Errors",
       x = "Estimator Type",
       y = "Log Squared Error") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("err_MLE" = "#d62828", "err_JSE" = "#778da9"))
\end{verbatim}

\subsection{R Code for Comparing with Ridge Regression}

\begin{verbatim}
# Simulate data
set.seed(1)
x <- matrix(rnorm(100*20), 100, 20)  # 100 observations, 20 predictors
beta <- runif(20, -2, 2)  # True coefficients
y <- x %*% beta + rnorm(100)  # Response variable

# Ridge regression
fit <- glmnet(x, y, alpha = 0)

# Plot shrinkage
plot(fit, xvar = "lambda", label = TRUE)
title(main = "Ridge Regression Shrinkage", line = -1)
\end{verbatim}