\section{Appendix}

\subsection{R Code for Implementation of the James-Stein Estimator}

\begin{verbatim}
# Function to apply James-Stein Estimator
james_stein_estimator <- function(z, sigma_squared) {
  n <- length(z)
  z_bar <- mean(z)
  s_squared <- sum((z - z_bar)^2) / n
  shrinkage_factor <- 1 - ((n-3) * sigma_squared / s_squared)
  shrinkage_factor <- max(0, shrinkage_factor)  # Ensure non-negative shrinkage
  return(z_bar + shrinkage_factor * (z - z_bar))
}

# Simulate data
set.seed(1)
z <- rnorm(10, mean = 5, sd = 1)
sigma_squared <- 1

# Apply the JSE
jse_results <- james_stein_estimator(z, sigma_squared)
print(jse_results)
\end{verbatim}

\subsection{R Code for Simulation of Estimator Effectiveness}

\begin{verbatim}
# Perform simulations and calculate MLE and JSE
simulate_estimators <- function(n, mu = 5, sigma = 1, num_sim = 1000) {
  mle_errors <- numeric(num_sim)
  jse_errors <- numeric(num_sim)
  
  for (i in 1:num_sim) {
    z <- rnorm(n, mean = mu, sd = sigma)
    mle <- mean(z)
    s_squared <- sum((z - mle)^2) / n
    shrinkage_factor <- 1 - ((n-3) * sigma^2 / s_squared)
    shrinkage_factor <- max(0, shrinkage_factor)
    jse <- mle + shrinkage_factor * (z - mle)
    
    mle_errors[i] <- sum((mle - mu)^2)
    jse_errors[i] <- sum((jse - mu)^2)
  }
  
  data.frame(
    Error = c(jse_errors, mle_errors),
    Estimator = rep(c("err_MLE", "err_JSE"), each = num_sim),
    N = rep(n, 2 * num_sim)
  )
}
\end{verbatim}

\subsection{R Code for Comparing with Ridge Regression}

\begin{verbatim}
# Simulate data
set.seed(1)
x <- matrix(rnorm(100*20), 100, 20)  # 100 observations, 20 predictors
beta <- runif(20, -2, 2)  # True coefficients
y <- x %*% beta + rnorm(100)  # Response variable

# Ridge regression using glmnet
fit <- glmnet(x, y, alpha = 0)

# Plot coefficient shrinkage
plot(fit, xvar = "lambda", label = TRUE)
title(main = "Ridge Regression Shrinkage", line = -1)
\end{verbatim}